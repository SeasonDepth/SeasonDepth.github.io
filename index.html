<!DOCTYPE html>
<html lang="en-US">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title text-align:"center">SeasonDepth Dataset</title>
<!--  <link rel="icon" href="files/favicon.ico">-->

  <script type="application/ld+json">
    {  "@context" : "http://schema.org",
          "@type" : "Dataset",
          "name" : "SeasonDepth",
          "description" : "This new monocular depth prediction dataset SeasonDepth with multi-traverse changing environments is generated through SfM and MVS pipeline filtering dynamic objects.",
          "temporalCoverage" : "2010/2011",
          "spatialCoverage" : "Pittsburg urban",
          "version" : "CC by-nc-sa 4.0",
          "identifier" : "doi:  10.6084/m9.figshare.14731323",
          "license" : "https://creativecommons.org/licenses/by-nc-sa/4.0/",
          "distribution" : {
            "@type" : "DataDownload",
            "encodingFormat" : "zip",
            "contentUrl" : "https://figshare.com/articles/dataset/SeasonDepth_Cross-Season_Monocular_Depth_Prediction_Dataset/14731323"
          },
          "sourceOrganization" : "Shanghai Jiao Tong University, Carnegie Mellon University",
          "datePublished" : "2021-06-05"
            }
    </script>


  <link rel="stylesheet" href="css/style.css">
  <link rel="stylesheet" href="css/zoomer.css">

  <script type="text/javascript" src="js/jquery-3.4.1.slim.min.js"></script>


</head>

<body>
  <div class="container-lg px-3 my-5 markdown-body">


    <h1 >
      SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments
    </h1>
      <p>
      <b>Authors</b>:
      <a href="https://hanjianghu.github.io/">Hanjiang Hu</a>,
      <a href="https://yangbaoquan.github.io/">Baoquan Yang</a>,
      <a href="https://qiaozhijian.github.io/">Zhijian Qiao</a>,
      <a href="https://scholar.google.com/citations?user=uirPPuYAAAAJ&hl=en">Ding Zhao</a>,
      <a href="http://scholar.google.com/citations?hl=en&user=q6AY9XsAAAAJ&hl=en">Hesheng Wang</a>
      <br>
      <b>Affiliations</b>:
      <a href="https://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>,
      <a href="https://www.cmu.edu/">Carnegie Mellon University</a>
    </p>

    <h2 >
      <br>
      Update Log
      </h2>
    <p>
      <li>
        <i>Nov., 2022 </i>:  The depth ground truth for <a href="http://seasondepth-challenge.org/index">SeasonDepth Prediction Challenge</a> test set has been released <a href="http://seasondepth-challenge.org/data/SeasonDepth_Challenge_test_depth_gt.zip">here</a> to boost the research community using <a href="http://seasondepth-challenge.org/leaderboard.html">leaderboard</a>. We are hosting <a href="https://robodepth.github.io/">the RoboDepth (Robust Out-of-distribution Depth prediction) Challenge</a>  at ICRA 2023. Stay tuned!
      </li>
      <li>
        <i>Jul., 2022 </i>:  The <a href="http://seasondepth-challenge.org/index">SeasonDepth Prediction Challenge</a> has been reopened for   <a href="https://iros2022.org/">IROS 2022</a> online Competition in October. Get ready to submit you results!
      </li>
      <li>
        <i>May, 2022 </i>: The  SeasonDepth Prediction Challenge and Workshop on Trustworthy Autonomy and Robotics was successfully held at ICRA 2022. Thanks all the speakers and congratulations to the winners on the final <a href="http://seasondepth-challenge.org/leaderboard_ICRA2022.html">leaderboard</a>!
      </li>
      <li>
        <i>Feb., 2022 </i>: The <a href="http://seasondepth-challenge.org/index">SeasonDepth Prediction Challenge</a> will be held at <a href="https://www.icra2022.org/">ICRA 2022</a> Competition in May and <a href="https://iros2022.org/">IROS 2022</a> online Competition in October.
      </li>
      <li>
        <i>Jan., 2022 </i>: The SeasonDepth Prediction Challenge is <a href="http://seasondepth-challenge.org/index">online</a> now! Sign up and participate in the competition!
      </li>
      <li>
        <i>Aug., 2021 </i>: SeasonDepth training set v1.1 and the fine-tuned models released, including 3 new slices for model fine-tuning.
      </li>
      <li>
        <i>Jul., 2021 </i>: SeasonDepth training set v1 and the fine-tuned models released.
      </li>
      <li>
        <i>Jun., 2021 </i>: SeasonDepth website and benchmark toolkit released along with validation set.
      </li>
    </p>

    <h2>
        <br>
      Introduction
   </h2>
    <p>
      <b>SeasonDepth</b> is a monocular depth prediction dataset that contains multi-traverse outdoor images from changing environments. It is the first depth prediction dataset with multi-environment road scenes to benchmark the depth prediction performance under different environmental conditions.
      See our <a href="https://arxiv.org/abs/2011.04408"><b>work</b></a> for more details and check out the <a href="https://github.com/SeasonDepth/SeasonDepth" >toolkit repo</a> for SeasonDepth benchmark. The dataset is built through structure from motion based on <a href="https://www.ri.cmu.edu/pub_files/2011/6/IV2011.pdf">CMU Visual Localization</a> and <a href="https://www.visuallocalization.net/datasets/">CMU-Seasons</a> dataset.
    </p>

    <p>
      <img  width="17%" src="images/scene1/demo_rgb_1.jpg">
      <img  width="17%" src="images/scene1/demo_rgb_2.jpg">
      <img  width="17%" src="images/scene1/demo_rgb_3.jpg">
      <img  width="17%" src="images/scene1/demo_rgb_4.jpg">
      <img  width="17%" src="images/scene1/demo_rgb_5.jpg"> <br>
        <img  width="17%" src="images/scene1/demo_dep_1.png">
      <img  width="17%" src="images/scene1/demo_dep_2.png">
      <img  width="17%" src="images/scene1/demo_dep_3.png">
      <img  width="17%" src="images/scene1/demo_dep_4.png">
      <img  width="17%" src="images/scene1/demo_dep_5.png"> <br>
        <img  width="17%" src="images/scene2/demo_rgb1.jpg">
      <img  width="17%" src="images/scene2/demo_rgb2.jpg">
      <img  width="17%" src="images/scene2/demo_rgb3.jpg">
      <img  width="17%" src="images/scene2/demo_rgb4.jpg">
      <img  width="17%" src="images/scene2/demo_rgb5.jpg"> <br>
        <img  width="17%" src="images/scene2/demo_dep1.png">
      <img  width="17%" src="images/scene2/demo_dep2.png">
      <img  width="17%" src="images/scene2/demo_dep3.png">
      <img  width="17%" src="images/scene2/demo_dep4.png">
      <img  width="17%" src="images/scene2/demo_dep5.png">
    </p>


    <h2 >
        <br>
      SeasonDepth Dataset
    </h2>

    <h3 id="download">
      Dataset Download
    </h3>
    <p>
      We have released the test (validation) splits of SeasonDepth for zero-shot evaluation, including RGB images and depth maps under twelve different environments. The released dataset is placed  in long-term preserved <i>figshare</i> with persistent identifier Digital Object Identifier (DOI) of <i>10.6084/m9.figshare.14731323</i>.
        The test (validation) set is available <a href="https://doi.org/10.6084/m9.figshare.14731323">HERE</a>. The training set of v1.1 has a DOI of <i>10.6084/m9.figshare.16442025</i> and is available <a href="https://doi.org/10.6084/m9.figshare.16442025"> HERE</a> together with the <a href="https://drive.google.com/file/d/12Xt_l26ZTCq2M2Oim1BfS2ltur6cX7H6/view?usp=sharing">fine-tuned models</a> on that, feel free to follow <a href="https://github.com/cogaplex-bts/bts">BTS</a> and <a href="https://github.com/ClementPinard/SfmLearner-Pytorch">SfMLearner</a> repos to fine-tune or evaluate them for benchmark. The detailed format and structure of test (validation) and training set can be found <a href="https://github.com/SeasonDepth/SeasonDepth/tree/master/dataset_info#seasondepth-test-and-training-set-for-benchmark">HERE</a>.
    </p>
      
    <h3 >
      Dataset Statistics
    </h3>
      <p>
          The distributions of relative depth value for all the environments are shown below after mean quartile alignment.
      <img src="images/all_env_map.png" width="80%">
      </p>
      <h3 >
      Dataset Details
    </h3>
    <p>
        The detailed information about partitioning, structure layout and image format can be found <a href="https://github.com/SeasonDepth/SeasonDepth/tree/master/dataset_info">HERE</a>.
    </p>


    <h2 >
        <br>
        SeasonDepth Benchmark
        </h2>
      <h3>
          Baseline Evaluation Results
      </h3>
    <p>
    Here we present the baseline performance of monocular depth estimation on the SeasonDepth dataset. Please refer to <a href="https://arxiv.org/abs/2011.04408">our paper</a> for more details. <br>
      <img src="images/results.png" width="80%">
      <img src="images/vis_results.png" width="80%">
    </p>

    <h3 >
       Benchmark Toolkit
     </h3>
    <p>
      Please visit our <a href="https://github.com/SeasonDepth/SeasonDepth">official repository</a> for SeasonDepth benchmark toolkit.
    </p>


    <h2 >
        <br>
        License
            </h2>
    <p>
    The SeasonDepth dataset, the toolkit code and fine-tuned models are under the license of <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/">BY-NC-SA 4.0</a>.
    </p>


    <h2 >
      <br>
      Citation
      </h2>
    <p>
      If you use the SeasonDepth dataset please cite all of the three references:
    </p>
    <div class="highlighter-rouge">
      <div class="highlight">
<pre class="highlight">
<code>@article{SeasonDepth,
  title={SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments},
  author={Hu, Hanjiang and Yang, Baoquan and Qiao, Zhijian and Zhao, Ding and Wang, Hesheng},
  journal={arXiv preprint arXiv:2011.04408},
  year={2021}
}

@inproceedings{Sattler2018CVPR,
  author={Sattler, Torsten and Maddern, Will and Toft, Carl and Torii, Akihiko and Hammarstrand, Lars and Stenborg, Erik and Safari, Daniel and Okutomi, Masatoshi and Pollefeys, Marc and Sivic, Josef and Kahl, Fredrik and Pajdla, Tomas},
  title={{Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions}},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018},
}

@inproceedings{badino2011visual,
  title={Visual topometric localization},
  author={Badino, Hern{\'a}n and Huber, Daniel and Kanade, Takeo},
  booktitle={2011 IEEE Intelligent Vehicles Symposium (IV)},
  pages={794--799},
  year={2011},
  organization={IEEE}
}
</code>
</pre>
      </div>
    </div>


    <h2 id="contact">
      <br>
      Contact
       </h2>
    <p>
      If you have any questions, please contact us through
      <a href="mailto:seasondepth@outlook.com">seasondepth@outlook.com</a>.
    </p>






    <br><br><br>

    <hr>
    <p>Last updated: <i>Aug. 28th, 2021 </i></p>
  </div>

